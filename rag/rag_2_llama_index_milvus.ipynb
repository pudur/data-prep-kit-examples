{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using llama Index with Milvus\n",
    "\n",
    "https://docs.llamaindex.ai/en/stable/examples/vector_stores/MilvusIndexDemo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20 chunks\n",
      "Document [0].doc_id: 82edc031-a8eb-4808-9058-cf84e4b3d4be\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "import pprint\n",
    "\n",
    "# load documents\n",
    "documents = SimpleDirectoryReader(\n",
    "    # input_dir = './data/10k/input/'\n",
    "    input_dir = 'data/granite-docs/input'\n",
    ").load_data()\n",
    "\n",
    "print (f\"Loaded {len(documents)} chunks\")\n",
    "\n",
    "print(\"Document [0].doc_id:\", documents[0].doc_id)\n",
    "# pprint.pprint (documents[0], indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sujee/apps/anaconda3/envs/data-prep-kit-dev-1/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name = \"BAAI/bge-small-en-v1.5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to vector db\n",
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from llama_index.vector_stores.milvus import MilvusVectorStore\n",
    "\n",
    "\n",
    "vector_store = MilvusVectorStore(\n",
    "    uri=\"./rag_llamaindex_milvus_demo.db\", dim=384, overwrite=True\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 972 ms, sys: 114 ms, total: 1.09 s\n",
      "Wall time: 1.56 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# create an index\n",
    "\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "## Load Settings from .env file\n",
    "from dotenv import find_dotenv, dotenv_values\n",
    "\n",
    "# _ = load_dotenv(find_dotenv()) # read local .env file\n",
    "config = dotenv_values(find_dotenv())\n",
    "\n",
    "os.environ[\"REPLICATE_API_TOKEN\"] = config.get('REPLICATE_API_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 452 ms, sys: 11.7 ms, total: 464 ms\n",
      "Wall time: 926 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# create an index\n",
    "\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['llamacollection']\n",
      "---------\n",
      "{'aliases': [],\n",
      " 'auto_id': False,\n",
      " 'collection_id': 0,\n",
      " 'collection_name': 'llamacollection',\n",
      " 'consistency_level': 0,\n",
      " 'description': '',\n",
      " 'enable_dynamic_field': True,\n",
      " 'fields': [{'description': '',\n",
      "             'field_id': 100,\n",
      "             'is_primary': True,\n",
      "             'name': 'id',\n",
      "             'params': {'max_length': 65535},\n",
      "             'type': <DataType.VARCHAR: 21>},\n",
      "            {'description': '',\n",
      "             'field_id': 101,\n",
      "             'name': 'embedding',\n",
      "             'params': {'dim': 384},\n",
      "             'type': <DataType.FLOAT_VECTOR: 101>}],\n",
      " 'num_partitions': 0,\n",
      " 'num_shards': 0,\n",
      " 'properties': {}}\n"
     ]
    }
   ],
   "source": [
    "# See data in vector db\n",
    "\n",
    "from pymilvus import MilvusClient\n",
    "import pprint \n",
    "\n",
    "client = MilvusClient('./rag_llamaindex_milvus_demo.db')\n",
    "res = client.list_collections()\n",
    "\n",
    "print(res)\n",
    "print ('---------')\n",
    "\n",
    "res = client.describe_collection(\n",
    "    collection_name=res[0]\n",
    ")\n",
    "\n",
    "pprint.pprint(res)\n",
    "# print (\"âœ… Connected to Milvus instance: ./rag_llamaindex_milvus_demo.db\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.replicate import Replicate\n",
    "from llama_index.core import Settings\n",
    "\n",
    "llm = Replicate(\n",
    "    model=\"meta/meta-llama-3-8b-instruct\",\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Based on the provided context information, it appears that the document is a report or paper related to the Granite Foundation Models, which are a set of pre-trained language models. The report discusses the evaluation and testing of these models, including their performance on various tasks and benchmarks.\n",
      "\n",
      "Here is a summary of the document:\n",
      "\n",
      "The report begins by introducing the Granite Foundation Models, which are a set of pre-trained language models designed for multilingual tasks. The models are evaluated on various benchmarks, including XL-sum, ML-sum, and XGLUE.\n",
      "\n",
      "The report then provides an overview of the evaluation process, including the metrics used to assess the models' performance. The metrics include FiQA-Opinion and Insurance QA metrics, which are used to evaluate the models' ability to generate accurate and informative summaries.\n",
      "\n",
      "The report also discusses the results of the evaluation, including the performance of the Granite Foundation Models on various tasks and benchmarks. The results show that the models perform well on most tasks, but there are some areas where they struggle.\n",
      "\n",
      "The report also includes a section on model safety and red-teaming benchmarks, which evaluates the models' ability to generate safe and responsible language. The results show that the models perform well on these tasks, but there are some areas where they need improvement.\n",
      "\n",
      "Finally, the report includes an appendix with additional information on the Granite Foundation Models, including their architecture, training data, and evaluation results.\n",
      "\n",
      "Overall, the document provides an overview of the Granite Foundation Models and their evaluation on various tasks and benchmarks. It also discusses the results of the evaluation and provides additional information on the models' architecture and training data.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "res = query_engine.query(\"Summarize this document for me\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Based on the provided context, the training dataset was IBM's curated pre-training dataset at the time of granite.13b.v2's training.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "res = query_engine.query(\"What was the training dataset?\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "I'm happy to help! However, I don't see any information about the moon landing in the provided context. The text appears to be about a language model called Granite and its training process, architecture, and infrastructure. There is no mention of the moon landing. If you could provide more context or clarify the query, I'd be happy to try and assist you further!\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "res = query_engine.query(\"When was the moon landing?\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-prep-kit-dev-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
